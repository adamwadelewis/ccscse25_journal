@article{einstein,
  author    = "Albert Einstein",
  title     = "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
            [{On} the electrodynamics of moving bodies]",
  journal   = "Annalen der Physik",
  volume    = "322",
  number    = "10",
  pages     = "891--921",
  year      = "1905",
  DOI       = "http://dx.doi.org/10.1002/andp.19053221004"
}

@book{latexcompanion,
  author    = "Michel Goossens and Frank Mittelbach and Alexander Samarin",
  title     = "The \LaTeX\ Companion",
  year      = "1993",
  publisher = "Addison-Wesley",
  address   = "Reading, Massachusetts"
}

@online{knuthwebsite,
  author    = "Donald Knuth",
  title     = "Knuth: Computers and Typesetting",
  note      = "\url{http://www-cs-faculty.stanford.edu/~uno/abcde.html}"
}

@online{meinke,
  author    = "John Meinke",
  title     = "Manuscript Formatting",
  note      = "\url{http://www.ccsc.org/wp-content/uploads/ManuscriptFormatting2015.pdf}"
}

@online{copyright,
  author    = "John Meinke",
  title     = "Copyright Release Form",
  note      = "\url{http://www.ccsc.org/wp-content/uploads/CopyrightRelease2015.pdf}"
}

@inproceedings{maurer,
  author    = {Maurer, Frank},
  title     = {Agile Methods and Interaction Design: Friend or Foe?},
  booktitle = {Proceedings of the 1st ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
  series    = {EICS '09},
  year      = {2009},
  isbn      = {978-1-60558-600-7},
  location  = {Pittsburgh, PA, USA},
  pages     = {209--210},
  numpages  = {2},
  url       = {http://doi.acm.org/10.1145/1570433.1570435},
  doi       = {10.1145/1570433.1570435},
  acmid     = {1570435},
  publisher = {ACM},
  address   = {New York, NY, USA},
  keywords  = {agile methods, interaction design},
}

@article{fang2025signllmsignlanguageproduction,
  publtype={informal},
  author={Sen Fang and Lei Wang and Ce Zheng and Yapeng Tian and Chen Chen},
  title={SignLLM: Sign Languages Production Large Language Models},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2405.10718},
  url={https://doi.org/10.48550/arXiv.2405.10718}
}

@inproceedings{wong2024signgpt,
title={Sign2{GPT}: Leveraging Large Language Models for Gloss-Free Sign Language Translation},
author={Ryan Wong and Necati Cihan Camgoz and Richard Bowden},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=LqaEEs3UxU}
}

@online{lingvano,
  author    = "Lingvano",
  note      = "\url{https://www.lingvano.com/asl/}"
}

@inproceedings{knapp,
author = {Knapp, Vaclav and Bohacek, Matyas},
title = {Pose-aware Large Language Model Interface for Providing Feedback to Sign Language Learners},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3688515},
doi = {10.1145/3663548.3688515},
abstract = {Sign language learners often find it challenging to self-identify and correct mistakes, and so many turn to automated methods that provide sign language feedback. However, they find that existing methods either require specialized equipment or lack robustness. They, therefore, have to seek human tutors or give up on the inquiry altogether. To overcome the barriers in accessibility and robustness, we build a large language model (LLM)-based tool for that provide feedback to sign language learners. The tool can analyze videos from diverse camera and background settings without specialized equipment thanks to a sign language segmentation and keyframe identification model. Using a pose-aware LLM, the tool can then produce feedback in written language. We present our tool as a demo web application, opening its implementation into specialized learning applications.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {121},
numpages = {5},
keywords = {Large Language Models, Learning Tool, Sign Language},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@article{signvision,
  author    = "Nasrullah Nazaruddin",
  title     = "SignVision — A Real-time Mobile Sign Language Recognition Application built using ChatGPT",
  journal = "Medium",
  year = "2024",
  month = "April",
  url      = "https://medium.com/@nesruler/signvision-a-real-time-mobile-sign-language-recognition-application-using-chatgpt-1ce73a0c6a55"
}

@online{aslapp,
  author    = "The ASL App",
  note      = "\url{https://theaslapp.com/}"
}


@article{orovwode,
  author    = "Hope Orovwode and Oduntan Ibukun and John Amanesi Abubakar",
  title     = "A machine learning-driven web application for sign language learning",
  journal   = "Frontiers in Artificial Intelligence",
  volume    = "7",
  day    = "10",
  month = "June",
  year      = "2024",
  DOI       = "10.3389/frai.2024.1297347"
}

@online{deafcenter,
  author    = "The National Deaf Center",
  note      = "\url{https://theaslapp.com/}"
}

U.S. Census Bureau. (2023). American Community Survey 1-year Public Use Microdata Samples [SAS Data file].  Retrieved from https://factfinder.census.gov/faces/nav/jsf/pages/searchresults.xhtml?refresh=t

@online{census,
  author    = "U.S Census Bureau",
  title     = "2023 American Community Survey",
  note      = "\url{https://data.census.gov/table/ACSST1Y2023.S1810}"
}

@article{mitchell,
  author    = "Ross E Mitchell and Travas A Young",
  title     = "How Many People Use Sign Language? A National Health Survey-Based Estimate",
  journal   = "The Journal of Deaf Studies and Deaf Educatione",
  volume    = "28",
  number    = "1",
  month = "January",
  year      = "2023",
  pages     = "1-6",
  DOI       = "https://doi.org/10.1093/deafed/enac031"
}

@online{mla,
  author    = "Modern Language Association",
  title     = "Enrollments in Languages Other Than En glish in United States Institutions of Higher Education, Summer 2016 and Fall 2016: Final Report",
  note      = "\url{https://www.mla.org/content/download/110154/file/2016-Enrollments-Final-Report.pdf}"
}

@article{mitchell0,
 ISSN = {03021475, 15336263},
 URL = {http://www.jstor.org/stable/26190621},
 abstract = {This article traces the sources of the estimates of the number of American Sign Language users in the United States. A variety of claims can be found in the literature and on the Internet, some of which have been shown to be unfounded but continue to be cited. In our search for the sources of the various (mis)understandings, we have found that all of the data-based estimates of the number of people who use ASL in the United States have their origin in a single study published in the early 1970s, which inquired about signing in general and not ASL use in particular. There has been neither subsequent research to update these estimates nor any specific study of ASL use. The article concludes with a call to action to rectify this problem.},
 author = {Ross E. Mitchell and Travas A. Young and Bellamie Bachleda and Michael A. Karchmer},
 journal = {Sign Language Studies},
 number = {3},
 pages = {306--335},
 publisher = {Gallaudet University Press},
 title = {How Many People Use ASL in the United States?: Why Estimates Need Updating},
 urldate = {2025-05-17},
 volume = {6},
 year = {2006}
}

@article{ALAGHBAND2023100504,
title = {A survey on sign language literature},
journal = {Machine Learning with Applications},
volume = {14},
pages = {100504},
year = {2023},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2023.100504},
url = {https://www.sciencedirect.com/science/article/pii/S2666827023000579},
author = {Marie Alaghband and Hamid Reza Maghroor and Ivan Garibay},
keywords = {Sign language recognition, Gesture recognition, Facial expression recognition, Sign language translation, Artificial intelligence, Machine learning, Deep learning, Hardware-based, Vision-based},
abstract = {Individuals with hearing impairment encounter various types and levels of difficulties, highlighting the need for more research to provide effective support. One significant difficulty is communication and interaction with others. Given that these individuals employ sign language as their primary mode of communication, there exists a notable information void among those who can hear in comprehending and interpreting sign language. Consequently, to bridge this gap, the field of sign language research has seen significant growth. In this study, we emphasize the importance of sign language recognition and translation and provide a comprehensive review of relevant research conducted in this field. Our examination encompasses multiple perspectives, including sign language recognition, translation, and the availability of datasets. By exploring these aspects, we aim to contribute to the advancement of sign language literature and its practical applications.}
}

@article{lomicka,
  author={Lara L. Lomicka},
  title={"To Gloss or Not To Gloss": An Investigation of Reading Comprehension Online"},
  year={1998},
  journal={Language Learning & Technology},
  volume={1},
  number={2},
  pages={41-50},

}

@inproceedings{hwang-etal-2025-efficient,
    title = "An Efficient Gloss-Free Sign Language Translation Using Spatial Configurations and Motion Dynamics with {LLM}s",
    author = "Hwang, Eui Jun  and
      Cho, Sukmin  and
      Lee, Junmyeong  and
      Park, Jong C.",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.197/",
    pages = "3901--3920",
    ISBN = "979-8-89176-189-6",
    abstract = "Gloss-free Sign Language Translation (SLT) converts sign videos into spoken language sentences without relying on glosses, which are the written representations of signs. Recently, Large Language Models (LLMs) have shown remarkable translation performance in gloss-free methods by harnessing their powerful natural language generation capabilities. However, these methods often rely on domain-specific fine-tuning of visual encoders to achieve optimal results. By contrast, we emphasize the importance of capturing the spatial configurations and motion dynamics in sign language. With this in mind, we introduce Spatial and Motion-based Sign Language Translation (SpaMo), a novel LLM-based SLT framework. The core idea of SpaMo is simple yet effective: instead of domain-specific tuning, we use off-the-shelf visual encoders to extract spatial and motion features, which are then input into an LLM along with a language prompt. Additionally, we employ a visual-text alignment process as a lightweight warm-up step before applying SLT supervision. Our experiments demonstrate that SpaMo achieves state-of-the-art performance on three popular datasets{---}PHOENIX14T, CSL-Daily, and How2Sign{---}without visual fine-tuning."
}

@inproceedings{feng2024chatpose,
  title = {{ChatPose}: Chatting about 3D Human Pose},
  booktitle = {IEEE/CVF Conf.~on Computer Vision and Pattern Recognition (CVPR)},
  abstract = {We introduce ChatPose, a framework employing Large Language Models (LLMs) to understand and reason about 3D human poses from images or textual descriptions. Our work is motivated by the human ability to intuitively understand postures from a single image or a brief description, a process that intertwines image interpretation, world knowledge, and an understanding of body language. Traditional human pose estimation and generation methods often operate in isolation, lacking semantic understanding and reasoning abilities. ChatPose addresses these limitations by embedding SMPL poses as distinct signal tokens within a multimodal LLM, enabling the direct generation of 3D body poses from both textual and visual inputs. Leveraging the powerful capabilities of multimodal LLMs, ChatPose unifies classical 3D human pose and generation tasks while offering user interactions. Additionally, ChatPose empowers LLMs to apply their extensive world knowledge in reasoning about human poses, leading to two advanced tasks: speculative pose generation and reasoning about pose estimation. These tasks involve reasoning about humans to generate 3D poses from subtle text queries, possibly accompanied by images. We establish benchmarks for these tasks, moving beyond traditional 3D pose generation and estimation methods. Our results show that ChatPose outperforms existing multimodal LLMs and task-specific methods on these newly proposed tasks. Furthermore, ChatPose's ability to understand and generate 3D human poses based on complex reasoning opens new directions in human pose analysis.},
  month = jun,
  year = {2024},
  slug = {chatpose},
  author = {Feng, Yao and Lin, Jing and Dwivedi, Sai Kumar and Sun, Yu and Patel, Priyanka and Black, Michael J.},
  url = {https://yfeng95.github.io/ChatPose/},
  month_numeric = {6}
}

@online{signlanguageai,
  author    = "Paul Kelly",
  title     = "Sign Language AI",
  note      = "\url{https://play.google.com/store/apps/details?id=ai.terp.www.twa&hl=en_US&pli=1}"
}

@online{chatGPT,
  author    = "OpenAI",
  title     = "ChatGPT",
  note      = "\url{chatgpt.com}"
}

@online{handtalk,
  author    = "Hand Talk",
  title     = "Hand Talk Translator",
  note      = "\url{https://play.google.com/store/apps/details?id=br.com.handtalk&hl=en_US}"
}

@online{firestore,
  author    = "Google",
  title     = "Firestore",
  note      = "\url{https://firebase.google.com/products/firestore}"
}

@online{pytorch,
  author    = "PyTorch Foundation",
  title     = "PyTorch",
  note      = "\url{https://pytorch.org/}"
}


@online{llama,
  author    = "Meta",
  title     = "Llama 3",
  note      = "\url{https://www.llama.com/models/llama-3/}"
}

https://www.nomic.ai/gpt4all
@online{gpt4all,
  author    = "Nomic",
  title     = "GPT4All",
  note      = "\url{https://www.nomic.ai/gpt4all}"
}

@article{monsen,
  author    = "Randall B. Monsen",
  title     = "The Oral Speech Intelligibility of Hearing-Impaired Talkers",
  journal   = "Journal of Speech and Hearing Disorders",
  volume    = "48",
  number    = "3",
  pages     = "286--296",
  year      = "1983",
  DOI       = "https://doi.org/10.1044/jshd.4803.2"
}

@Article{gozalbo,
AUTHOR = {Porcar-Gozalbo, Nadia and López-Zamora, Miguel and Valles-González, Beatriz and Cano-Villagrasa, Alejandro},
TITLE = {Impact of Hearing Loss Type on Linguistic Development in Children: A Cross-Sectional Study},
JOURNAL = {Audiology Research},
VOLUME = {14},
YEAR = {2024},
NUMBER = {6},
PAGES = {1014--1027},
URL = {https://www.mdpi.com/2039-4349/14/6/84},
PubMedID = {39727607},
ISSN = {2039-4349},
ABSTRACT = {Background/Objectives: Hearing loss in childhood is associated with significant challenges in linguistic and cognitive development, particularly affecting language skills such as syntax, semantics, and pragmatics, which are essential for effective communication and social integration. This study aimed to analyze how different types and degrees of hearing loss impact linguistic development in children, and to identify clinical factors—such as age at diagnosis and years of language intervention—that may predict language performance. Methods: This study included a sample of 140 children aged 6 to 12, categorized into seven groups based on their hearing condition: unilateral and bilateral conductive, unilateral and bilateral sensorineural, unilateral and bilateral mixed hearing loss, and a control group with no hearing loss. Linguistic development was assessed using the Clinical Evaluation of Language Fundamentals-5 (CELF-5), a validated tool for diagnosing language disorders. Statistical analyses, including MANOVA and multiple regression, were conducted to evaluate differences in linguistic skills across groups and to determine the predictive value of clinical variables on total language performance. Results: The analysis revealed statistically significant differences across groups in all assessed linguistic domains (p < 0.001), with children with severe or bilateral hearing loss exhibiting notably lower scores compared to normohearing peers. The multiple regression analysis indicated that type of hearing loss was the strongest predictor of total linguistic performance (β = −0.674), followed by age at diagnosis (β = −0.285) and age of hearing device adaptation (β = −0.220). Years of language intervention also contributed significantly (β = 0.198) but to a lesser extent. Conclusions: This study highlights the critical impact of early and comprehensive auditory and language intervention on linguistic outcomes for children with hearing impairments. Early diagnosis and timely adaptation of hearing aids or cochlear implants are essential in mitigating language deficits, particularly in areas like syntax and pragmatic skills. These findings support the need for specialized, long-term interventions tailored to the severity and type of hearing loss to improve language development in this population.},
DOI = {10.3390/audiolres14060084}
}

@Article{scott,
AUTHOR = {Scott, Jessica A. and Dostal, Hannah M.},
TITLE = {Language Development and Deaf/Hard of Hearing Children},
JOURNAL = {Education Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {2},
ARTICLE-NUMBER = {135},
URL = {https://www.mdpi.com/2227-7102/9/2/135},
ISSN = {2227-7102},
ABSTRACT = {This article explores the available research literature on language development and language interventions among deaf and hard of hearing (d/hh) children. This literature is divided into two broad categories: Research on natural languages (specifically American Sign Language and spoken English) and research on communication systems (specifically iterations of signed English and cued speech). These bodies of literature are summarized, with special attention paid to intervention research and research exploring the impacts of language skills on literacy development. Findings indicate that there is generally a stronger research base on natural languages as compared to communication systems, though more studies in both categories are necessary. Additionally, there are very few intervention studies and even fewer that aim to intervene upon language with the explicit goal of impacting literacy; therefore, there is little known about whether and how interventions that aim to support language development may have direct or indirect impacts on literacy within this population. Further research on this topic, as well as replication studies and research with larger sample sizes, is strongly recommended.},
DOI = {10.3390/educsci9020135}
}

@inproceedings{li2020word,
    title={Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison},
    author={Li, Dongxu and Rodriguez, Cristian and Yu, Xin and Li, Hongdong},
    booktitle={The IEEE Winter Conference on Applications of Computer Vision},
    pages={1459--1469},
    year={2020}
}


@online{mediapipe,
  author    = "Google",
  title     = "MediaPipe",
  note      = "\url{https://ai.google.dev/edge/mediapipe/solutions/guide}"
}

@article{tprune,
author = {Mao, Jiachen and Yang, Huanrui and Li, Ang and Li, Hai and Chen, Yiran},
title = {TPrune: Efficient Transformer Pruning for Mobile Devices},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2378-962X},
url = {https://doi.org/10.1145/3446640},
doi = {10.1145/3446640},
abstract = {The invention of Transformer model structure boosts the performance of Neural Machine Translation (NMT) tasks to an unprecedented level. Many previous works have been done to make the Transformer model more execution-friendly on resource-constrained platforms. These researches can be categorized into three key fields: Model Pruning, Transfer Learning, and Efficient Transformer Variants. The family of model pruning methods are popular for their simplicity in practice and promising compression rate and have achieved great success in the field of convolution neural networks (CNNs) for many vision tasks. Nonetheless, previous Transformer pruning works did not perform a thorough model analysis and evaluation on each Transformer component on off-the-shelf mobile devices. In this work, we analyze and prune transformer models at the line-wise granularity and also implement our pruning method on real mobile platforms. We explore the properties of all Transformer components as well as their sparsity features, which are leveraged to guide Transformer model pruning. We name our whole Transformer analysis and pruning pipeline as TPrune. In TPrune, we first propose Block-wise Structured Sparsity Learning (BSSL) to analyze Transformer model property. Then, based on the characters derived from BSSL, we apply Structured Hoyer Square (SHS) to derive the final pruned models. Comparing with the state-of-the-art Transformer pruning methods, TPrune is able to achieve a higher model compression rate with less performance degradation. Experimental results show that our pruned models achieve 1.16\texttimes{}–1.92\texttimes{} speedup on mobile devices with 0\%–8\% BLEU score degradation compared with the original Transformer model.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = apr,
articleno = {26},
numpages = {22},
keywords = {real-time system, neural networks, model pruning, mobile computing, embedded software, Neural machine translation}
}

@ARTICLE{jinPrune,
  author={Jin, Yuyang and Zhong, Runxin and Long, Saiqin and Zhai, Jidong},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={Efficient Inference for Pruned CNN Models on Mobile Devices With Holistic Sparsity Alignment}, 
  year={2024},
  volume={35},
  number={11},
  pages={2208-2223},
  keywords={Computational modeling;Mobile handsets;Convolutional neural networks;Neural networks;Accuracy;Convolution;Optimization;Mobile devices;pruning;sparse neural network;code generation},
  doi={10.1109/TPDS.2024.3462092}}


@online{flutter,
  author    = "Google",
  title     = "Flutter",
  note      = "\url{https://flutter.dev/}"
}

@online{nomicai,
  author    = "Nomic AI",
  title     = "GPT4All",
  note      = "\url{https://www.nomic.ai/gpt4all/}"
}


@online{nomicai,
  title     = "ngrok",
  note      = "\url{https://ngrok.com/}"
}

@online{fastapi,
  title     = "FastAPI",
  note      = "\url{https://fastapi.tiangolo.com/}"
}